{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09625657",
   "metadata": {},
   "source": [
    "# Connecting the Power Spectrum, Structure Function, and decorrelation Scale\n",
    "\n",
    "**Purpose**: Code for finding the connection between the frequency spectrum, structure function, and the decorrelation scale.  \n",
    "\n",
    "**Luke Colosi | lcolosi@ucsd.edu**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0e75cf",
   "metadata": {},
   "source": [
    "## Theoretical Background\n",
    "\n",
    "For my research project, I want to estimate the dominant length and time scales that characterize the fluid flow and its tracers. By dominant, I am looking for the scale of structures that have the most energy or variance associated with them. However, there can be a board spectrum of structures of present in fluid which all have varying levels of energy. Therefore, I want to estimate the spectra structure of physical processes oocuring at the range of scales. In the case of a spectrum of scales, I want to better understand which scale the decorrelation scale picks out. \n",
    "\n",
    "Thus, my research question is: *How is the power spectrum related to the structure function and how does the decorrelation function relate to both of these spectral quantities?*\n",
    "\n",
    "Let's begin by defining each of these quantities: \n",
    "\n",
    "1. Power Spectrum: describes how the variance of a real-valued, stationary physical signal is distributed across positive frequencies. It essentially tells you how much of the signal's power is associated with oscillations at different frequencies. It is defined as: \n",
    "\n",
    "$$\n",
    "E(f) = \\langle |X(f)|^2 \\rangle\n",
    "$$\n",
    "\n",
    "where the averaging corresponds to the averaging over the $n$ fourier transformed segments following the Welch method and X(f) is the complex fourier coefficients defined as:\n",
    "\n",
    "$$\n",
    "X(f) = \\int^{\\infty}_{-\\infty} x(t)e^{-i2\\pi ft} dt\n",
    "$$\n",
    "\n",
    "2. Structure Function: \n",
    "\n",
    "$$\n",
    "S_n(\\textbf{r}) = \\langle |u(x + \\textbf{r}) - u(\\textbf{r})|^n \\rangle \n",
    "$$\n",
    "\n",
    "3. Autocorrelation Function and the Decorrelation Scale "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dbb3e2",
   "metadata": {},
   "source": [
    "Force matplotlib plots to display directly within the output cell of the notebook: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13436724",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383646be",
   "metadata": {},
   "source": [
    "\n",
    "Import python libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c990a00b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "# Set path to access python functions\n",
    "sys.path.append('/Users/lukecolosi/Desktop/projects/graduate_research/Gille_lab/AirSeaCoupling/tools/')\n",
    "\n",
    "#--- Luke's Python Functions ---# \n",
    "#from autocorr import compute_decor_scale, compute_autocorr\n",
    "#from "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50bf075",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- 1D Power Spectrum with the Welch Method ---# \n",
    "def spectrum1D_frequency(data, dt, M, units):\n",
    "\n",
    "    \"\"\"\n",
    "    Function for computing the 1D power density spectrum with the Welch method.\n",
    "    This function is written notationally for time series, but can be applied to spatial data.\n",
    "    The 1D frequency spectrum is computed by Hanning windowing segments of the data array with 50% overlap.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    data : Time or spatial data series. Data must be evenly spaced (NaNs must be interpolated).\n",
    "    dt : Time or spatial interval between measurements.\n",
    "    M : Number of windows.\n",
    "    units : 'Hz' (cyclical frequency) or 'rad/s' (radian frequency).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    psd : Normalized power spectral density function.\n",
    "    f : Frequency in units specified by units variable.\n",
    "    CI : 95% confidence interval.\n",
    "    variance : Dictionary containing the variance in the time and frequency domains.\n",
    "    \"\"\"\n",
    "\n",
    "    # Import libraries\n",
    "    import numpy as np\n",
    "    from spectra import spectral_uncer\n",
    "    from scipy.signal import hann, detrend\n",
    "\n",
    "    ###########################################################################\n",
    "    ## STEP #1 - Set fundamental parameters for computing spectrum\n",
    "    ###########################################################################\n",
    "\n",
    "    N = len(data)                 # Number of data points of entire time series\n",
    "    p = N // M                    # Number of data points within a window\n",
    "\n",
    "    # Compute frequency resolution\n",
    "    if units == 'Hz':\n",
    "        df = 1 / (p * dt)\n",
    "    elif units == 'rad/s':\n",
    "        df = 2 * np.pi / (p * dt)\n",
    "\n",
    "    # Compute number of positive frequencies\n",
    "    if p % 2 == 0:\n",
    "        L = p // 2 + 1\n",
    "    else:\n",
    "        L = (p - 1) // 2\n",
    "\n",
    "    # Compute the period of the fundamental frequency (lowest frequency)\n",
    "    T = p * dt\n",
    "\n",
    "    # Compute frequency vector (units: Hz or rad/s)\n",
    "    if p % 2 == 0:\n",
    "        if units == 'Hz':\n",
    "            f = (1 / T) * np.arange(0, p // 2 + 1)\n",
    "        elif units == 'rad/s':\n",
    "            f = (2 * np.pi / T) * np.arange(0, p // 2 + 1)\n",
    "    else:\n",
    "        if units == 'Hz':\n",
    "            f = (1 / T) * np.arange(0, (p - 1) // 2)\n",
    "        elif units == 'rad/s':\n",
    "            f = (2 * np.pi / T) * np.arange(0, (p - 1) // 2)\n",
    "\n",
    "    ###########################################################################\n",
    "    ## STEP #2 - Segment data with 50% overlap\n",
    "    ###########################################################################\n",
    "\n",
    "    nseg = M + M - 1              # Compute number of segments including 50% overlap\n",
    "\n",
    "    # Initialize array for splitting time series into windows with 50% overlap\n",
    "    data_seg_n = data[:M*p].reshape((p, M), order='F')  # Segment original data set\n",
    "\n",
    "    data_seg_50 = []\n",
    "    for iseg in range(M - 1):\n",
    "        ind_i = int(p * iseg + (p / 2))\n",
    "        ind_f = int(ind_i + p)\n",
    "        if ind_f <= len(data):\n",
    "            data_seg_50.append(data[ind_i:ind_f])\n",
    "\n",
    "    if data_seg_50:\n",
    "        data_seg_50 = np.stack(data_seg_50, axis=1)\n",
    "        data_seg_n = np.concatenate((data_seg_n, data_seg_50), axis=1)\n",
    "\n",
    "    ###########################################################################\n",
    "    ## STEP #3 - Remove linear trend for each segment and apply hanning window\n",
    "    ###########################################################################\n",
    "\n",
    "    # Obtain a hanning window:\n",
    "    window = hann(p) * np.sqrt(p / np.sum(hann(p)**2))\n",
    "\n",
    "    # Preallocate windowed detrended segmented data array\n",
    "    data_seg_w = np.zeros_like(data_seg_n)\n",
    "\n",
    "    for iseg in range(data_seg_n.shape[1]):\n",
    "        data_seg_w[:, iseg] = detrend(data_seg_n[:, iseg]) * window\n",
    "\n",
    "    ###########################################################################\n",
    "    ## STEP #4 - Compute mean 1D frequency spectrum\n",
    "    ###########################################################################\n",
    "\n",
    "    spec_sum = np.zeros(p)                 # Preallocate spectrum summation array\n",
    "    cn = np.zeros(p)                       # Preallocate counter\n",
    "    variance = {'time': np.zeros(nseg)}   # Preallocate variance in time domain\n",
    "\n",
    "    for iseg in range(nseg):\n",
    "        fft_data_seg = np.fft.fft(data_seg_w[:, iseg])          # Fourier transform data\n",
    "        amp = np.abs(fft_data_seg)**2                           # Compute amplitudes\n",
    "        amp_norm = amp / (p**2) / df                            # Normalize amplitudes\n",
    "\n",
    "        variance['time'][iseg] = np.var(data_seg_w[:, iseg])    # Variance in time domain\n",
    "\n",
    "        spec_sum += amp_norm                                    # Sum spectrum\n",
    "        cn += 1                                                 # Update counter\n",
    "\n",
    "    m_spec = spec_sum / cn                                      # Compute mean spectrum\n",
    "    psd = m_spec[:L]                                            # Grab positive frequencies\n",
    "\n",
    "    # Double the amplitude for positive frequencies to conserve variance\n",
    "    if N % 2 == 0:\n",
    "        psd[1:-1] *= 2\n",
    "    else:\n",
    "        psd[1:] *= 2\n",
    "\n",
    "    # Compute the variance in frequency space\n",
    "    variance['freq'] = np.sum(psd * df)\n",
    "\n",
    "    # Compute 95% confidence interval\n",
    "    CI = spectral_uncer(M, 0.05, psd)\n",
    "\n",
    "    return psd, f, CI, variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4e3eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#--- Autocorrelation and Autocovariance Function ---# \n",
    "def compute_autocorr(data, x, lag, bias, norm = 0):\n",
    "\n",
    "    \"\"\"\n",
    "    rho_pos, rho_neg, R_pos, R_neg, x_ref_pos, x_ref_neg = compute_autocorr(data, x, lag, task, bias, norm = 0)\n",
    "\n",
    "    Function for computing the autocovariance and autocorrelation \n",
    "    functions for positive and negative lag.\n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "        data : array\n",
    "            Time or spatial series of data. This data must be preprocessed in the following ways:\n",
    "            1) Detrend to remove any signal that is undesirable in the autocovariance function.\n",
    "            2) Missing data gaps are filled with NaN values to ensure a continuous time series.\n",
    "            3) Flagged data should be replaced with NaN values.\n",
    "            \n",
    "        lag : int\n",
    "            The desired number of lags for computing the correlation. The specified amount of lags is dependent\n",
    "            on the length of the time series. You want to set the amount of lags to a value where the \n",
    "            correlation coefficent is for the proper amount of iterations along to fixed time series.\n",
    "            Ex: lag_dt = len(data) (compute correlation coefficient at lag decreasing by one measurement at a time).\n",
    "            \n",
    "        bias : str\n",
    "            Specifies whether the covariance is biased or unbiased. The unbiased estimate is normalized by\n",
    "            1/n-m whereas the biased estimate is normalized by 1/n where n = total number of data points in time\n",
    "            series and m is the lag time from 0 lag. Furthermore, specifies whether the correlation coefficent is biased \n",
    "            or unbaised using the same normalizations in numerator (unbiased (normalized by 1/n-m) or biased\n",
    "            (normalized by 1/n)) and the normalization 1/n for both cases in the demominator. \n",
    "            Options: 'biased' or 'unbiased'.\n",
    "\n",
    "        norm : int\n",
    "            Specifies which lagged covariance you want to normalize the autocovariance function by. The normal convention\n",
    "            is to normalize it by the variance of the data record (the zeroth lag). However in the case where the noise in the\n",
    "            measurements is causing a large drop in the autocorrelation from the zeroth lag to the first lag (introducing \n",
    "            a decorrelation signal different from the decorrelation from the natural variability of the system), normalizing the\n",
    "            autocovariance function by the first lag will provide a more accurate decorrelation scale. Options includes 0 or 1 \n",
    "            corresponds to the zero and first lag respectively. Default value: norm = 0. \n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        rho_pos : array\n",
    "            Positive lag autocorrelation function.\n",
    "            \n",
    "        rho_neg : array\n",
    "            Negative lag autocorrelation function.\n",
    "            \n",
    "        R_pos : array\n",
    "            Positive lag autocovariance function.\n",
    "            \n",
    "        R_neg : array\n",
    "            Negative lag autocovariance function.\n",
    "\n",
    "        x_ref_pos : array\n",
    "            Lag variable for positive lag autocorrelation or autocovariance functions. \n",
    "\n",
    "        x_ref_neg : array \n",
    "            Lag variable for positive lag autocorrelation or autocovariance functions. \n",
    "\n",
    "        Libraries necessary to run function\n",
    "        -----------------------------------\n",
    "        import numpy as np \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Import libraries \n",
    "    import numpy as np\n",
    "\n",
    "    # Choose interval length n which the correlation coefficient will be computed (Counting and discouting masked elements)\n",
    "    N = len(data)\n",
    "    N_eff = np.sum(~data.mask)\n",
    "\n",
    "    # Initialize autocovariance and autocorrelation arrays\n",
    "    R         = np.zeros(lag)   # Autocovariance\n",
    "    rho       = np.zeros(lag)   # Autocorrelation \n",
    "    c_pairs_m  = np.zeros(lag)\n",
    "    c_pairs_nm = np.zeros(lag)\n",
    "\n",
    "    # Set the normalization factor for the autocorrelation \n",
    "\n",
    "    # Normalize by the zeroth lag \n",
    "    if norm == 0:\n",
    "\n",
    "            # Set zero lagged data segments\n",
    "            running = data[0:N]\n",
    "            fix = data[:N-0]\n",
    "\n",
    "            # Remove mean from each segment before computing covariance and correlation\n",
    "            fix -= np.ma.mean(data)\n",
    "            running -= np.ma.mean(data)\n",
    "\n",
    "            # Compute the normalization\n",
    "            Rnorm = (1/N_eff) * np.ma.dot(fix,running) #np.sum(data * np.conj(data))\n",
    "\n",
    "    #--- Note ---# \n",
    "    # The normalization is the same for the biased, unbiased, and unbiased estimates when normalizing with the zeroth lag\n",
    "    #  because the factors in front of the inner product are equivalent: \n",
    "    # \n",
    "    #       1/n_eff = (1 / n_eff) * (n / N) =  1/N_eff\n",
    "    # \n",
    "    # This is because n_eff = N_eff and n = N at tau = 0. Recall that: \n",
    "    # \n",
    "    #                    n = N - k,\n",
    "    # \n",
    "    # _eff denotes that the quantity excludes masked elements, and upper case N denotes that length of the full record\n",
    "    # while lower case n denotes the length of the lagged record. \n",
    "    \n",
    "    # Normalize by the first lag \n",
    "    elif norm == 1: \n",
    "\n",
    "        # Set lagged data segments\n",
    "        running = data[1:N]\n",
    "        fix = data[:N-1]\n",
    "\n",
    "        # Remove mean from each segment before computing covariance and correlation\n",
    "        fix -= np.ma.mean(data)\n",
    "        running -= np.ma.mean(data)\n",
    "\n",
    "        # Compute number of data pairs discounting pairs with masked vaules\n",
    "        combined_mask = np.logical_or(fix.mask, running.mask)\n",
    "        n_eff = np.sum(~combined_mask)\n",
    "        n = len(running)\n",
    "        \n",
    "        #--- Unbiased ---# \n",
    "        if bias == 'unbiased':\n",
    "            Rnorm = (1/n_eff) * np.ma.dot(fix,running) # np.sum(data[1:N] * np.conj(data[:N-1]))\n",
    "\n",
    "        if bias == 'unbiased_tapered':\n",
    "            Rnorm = (1/n_eff) * (n / N) * np.ma.dot(fix,running) # np.sum(data[1:N] * np.conj(data[:N-1]))\n",
    "\n",
    "        #--- Biased ---# \n",
    "        if bias == 'biased': \n",
    "            Rnorm = (1/N_eff) * np.ma.dot(fix,running) # np.sum(data[1:N] * np.conj(data[:N-1]))\n",
    "\n",
    "    # Loop through each lag interval to compute the correlation and covariance    \n",
    "    for k in range(lag):\n",
    "\n",
    "        # Set lagged data segments\n",
    "        running = data[k:N]\n",
    "        fix = data[:N-k]\n",
    "        \n",
    "        # Remove mean from each segment before computing covariance and correlation\n",
    "        fix -= np.ma.mean(data)\n",
    "        running -= np.ma.mean(data)\n",
    "\n",
    "        # Compute the correlation coefficient terms at lag k\n",
    "        inner_product = np.ma.dot(fix, running)\n",
    "\n",
    "        # Compute the number of data pairs counting pairs with masked values\n",
    "        n = len(running)  # Equivalent to N - k\n",
    "        c_pairs_m[k] = n\n",
    "\n",
    "        # Compute number of data pairs NOT counting pairs with masked vaules\n",
    "        combined_mask = np.logical_or(fix.mask, running.mask)\n",
    "        n_eff = np.sum(~combined_mask)\n",
    "        c_pairs_nm[k] = n_eff\n",
    "        \n",
    "        # Compute autocorrelation and autocovariance function at lag k\n",
    "\n",
    "        #--- Unbiased ---# \n",
    "        if bias == 'unbiased':\n",
    "            R[k] = (1 / n_eff) * inner_product\n",
    "            rho[k] = R[k] / Rnorm\n",
    "\n",
    "        #--- Unbiased Tapered (Triangular taper) ---# \n",
    "        if bias == 'unbiased_tapered':\n",
    "            R[k] = (1 / n_eff) * (n / N) * inner_product\n",
    "            rho[k] = R[k] / Rnorm\n",
    "\n",
    "        #--- Biased ---# \n",
    "        elif bias == 'biased':\n",
    "            R[k] = (1 / N_eff) * inner_product\n",
    "            rho[k] = R[k] / Rnorm\n",
    "    \n",
    "    # Combine positive and negative lag autocorrelation and autocovariance and set the lag vector\n",
    "    if norm == 0:\n",
    "\n",
    "        #--- Lag ---#  \n",
    "        x_ref_pos = x - x[0]\n",
    "        x_ref_neg = -1 * np.flip(x_ref_pos)[:-1]\n",
    "\n",
    "        #--- Autocovariance ---# \n",
    "        R_pos = R\n",
    "        R_neg = np.flip(R)[:-1]\n",
    "\n",
    "        #--- Autocorrelation ---# \n",
    "        rho_pos = rho\n",
    "        rho_neg = np.flip(rho)[:-1]\n",
    "\n",
    "    elif norm == 1:\n",
    "\n",
    "        #--- Lag ---#  \n",
    "        x_ref_pos = x - x[0]\n",
    "        x_ref_neg = -1 * np.flip(x_ref_pos)[:-1]\n",
    "\n",
    "        #--- Autocovariance ---# \n",
    "        R_pos = R\n",
    "        R_neg = np.flip(R)[:-1]\n",
    "\n",
    "        #--- Autocorrelation ---# \n",
    "        rho_pos = np.insert(rho[1:], 0, 1) # Set zero lag to unity \n",
    "        rho_neg = np.flip(rho)[:-1]\n",
    "    \n",
    "    return rho_pos, rho_neg, R_pos, R_neg, x_ref_pos, x_ref_neg #, c_pairs_m, c_pairs_nm (ignoring this output)\n",
    "\n",
    "\n",
    "#--- Decorrelation Scale Analysis ---%\n",
    "def compute_decor_scale(autocorr,x_ref,dx,bias,norm):\n",
    "\n",
    "    \"\"\"\n",
    "    Computes the decorrelation scale as an intergral time scale from the positively lag autocorrelation function.  \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    autocorr : array\n",
    "            Positive lag autocorrelation function. \n",
    "\n",
    "    x_ref : array \n",
    "            Lag time or distance independent variable. \n",
    "\n",
    "    dx : float \n",
    "            The distance between data points in physical space. \n",
    "\n",
    "    bias : str\n",
    "            Specifies whether the covariance is biased or unbiased. The unbiased estimate is normalized by\n",
    "            1/n-m whereas the biased estimate is normalized by 1/n where n = total number of data points in time\n",
    "            series and m is the lag time from 0 lag. Options: 'biased' or 'unbiased'.\n",
    "\n",
    "    norm : int\n",
    "            Specifies which lagged covariance you want to normalize the autocovariance function by. The normal convention\n",
    "            is to normalize it by the variance of the data record (the zeroth lag). However in the case where the noise in the\n",
    "            measurements is causing a large drop in the autocorrelation from the zeroth lag to the first lag (introducing \n",
    "            a decorrelation signal different from the decorrelation from the natural variability of the system), normalizing the\n",
    "            autocovariance function by the first lag will provide a more accurate decorrelation scale. Options includes 0 or 1 \n",
    "            corresponds to the zero and first lag respectively. Default value: norm = 0. \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    scale : float \n",
    "        The integral time scale estimate of the decorrelation scale. \n",
    "\n",
    "    Libraries necessary to run function\n",
    "    -----------------------------------\n",
    "    import numpy as np \n",
    "    from scipy.integrate import trapezoid\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    # Import libraries \n",
    "    import numpy as np\n",
    "    from scipy.integrate import trapezoid\n",
    "    \n",
    "    # Normalize by the zeroth lag \n",
    "    if norm == 0: \n",
    "\n",
    "        # Set the positive and negative lagged autocovariance functions\n",
    "        autocorr_pos = autocorr\n",
    "        autocorr_neg = np.flip(autocorr)[:-1]\n",
    "\n",
    "        # Set the positive and negative lag variable\n",
    "        x_ref_pos = x_ref\n",
    "        x_ref_neg = -1 * np.flip(x_ref)[:-1]\n",
    "\n",
    "        # Set the length of data series and data interval\n",
    "        N = len(autocorr_pos)      # length of one-sided autocorrelation function (and number of samples in data record)\n",
    "        R = N * dx                 # length of the data series (units of time or space)\n",
    "\n",
    "        # Initialize scale_N variable\n",
    "        scale_N = np.zeros(N)\n",
    "\n",
    "        # Loop through lags \n",
    "        for i in range(N):\n",
    "\n",
    "                # Index autocorrelation function \n",
    "\n",
    "                #--- Zeroth lag ---# \n",
    "                if i == 0:\n",
    "\n",
    "                        # Set interal of the autocorrelation function to zero (intergration range vanishes)\n",
    "                        iscale = 0  \n",
    "\n",
    "                #--- Higher Order lag ---#     \n",
    "                else:\n",
    "\n",
    "                        # Index the autocorrelation function and combine the negative and positive lagged autocorrelation functions \n",
    "                        coef = np.concatenate((autocorr_neg[N-i-1:], autocorr_pos[:i+1]))\n",
    "                        x    = np.concatenate((x_ref_neg[N-i-1:], x_ref_pos[:i+1]))\n",
    "\n",
    "                        # Compute time or spatial lag\n",
    "                        r = i * dx\n",
    "\n",
    "                        # Compute integral of autocorrelation function (with triangular filter weighting lower lags for unbiased estimator)\n",
    "                        if bias == 'unbiased':\n",
    "                                iscale = trapezoid((1 - (r / R)) * coef, x, dx=dx)\n",
    "                        elif bias == 'biased':\n",
    "                                iscale = trapezoid(coef, x, dx=dx)\n",
    "\n",
    "                # Save the ith scaling factor=\n",
    "                scale_N[i] = iscale\n",
    "\n",
    "        # Find the maximum decorrelation time scale (conservative estimate)\n",
    "        scale = np.nanmax(scale_N)\n",
    "\n",
    "    #--- Normalize by the first lag ---#\n",
    "    elif norm == 1: \n",
    "\n",
    "        # Set the positive lagged autocovariance functions\n",
    "        autocorr_pos = autocorr\n",
    "\n",
    "        # Set the length of data series and data interval\n",
    "        N = len(autocorr_pos)  # length of one-sided autocorrelation function\n",
    "        R = N * dx             # length of the data series\n",
    "\n",
    "        # Initialize scale_N variable\n",
    "        scale_N = np.zeros(N)\n",
    "\n",
    "        # Loop through lags \n",
    "        for i in range(N):\n",
    "\n",
    "                # Index autocorrelation function \n",
    "\n",
    "                #--- Zeroth and first lag ---# \n",
    "                if i == 0:\n",
    "\n",
    "                        # Set interal of the autocorrelation function to zero (intergration range vanishes)\n",
    "                        iscale = 0  \n",
    "\n",
    "                #--- Higher Order lag ---#     \n",
    "                else:\n",
    "\n",
    "                        # Index the autocorrelation function and combine the negative and positive lagged autocorrelation functions \n",
    "                        autocor_pos_lag = autocorr_pos[:i+1]\n",
    "\n",
    "                        # Index the autocorrelation function \n",
    "                        x_ref_pos_lag = x_ref[:i+1]\n",
    "\n",
    "                        # Compute spatial lag\n",
    "                        r = i * dx\n",
    "\n",
    "                        # Compute integral of autocorrelation function (using the symmetry across the y-axis)\n",
    "                        if bias == 'unbiased': \n",
    "                               iscale_pos = trapezoid((1 - (r / R)) * autocor_pos_lag, x_ref_pos_lag, dx=dx) \n",
    "                        elif bias == 'biased': \n",
    "                               iscale_pos = trapezoid(autocor_pos_lag, x_ref_pos_lag, dx=dx) \n",
    "                        iscale = 2*iscale_pos\n",
    "\n",
    "                # Save the ith scaling factor\n",
    "                scale_N[i] = iscale\n",
    "\n",
    "\n",
    "        # Find the maximum decorrelation time scale (conservative estimate)\n",
    "        scale = np.nanmax(scale_N)\n",
    "\n",
    "    return scale "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29236edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_structure_function(field, max_lag, orders=[1, 2, 3, 4]):\n",
    "    \"\"\"\n",
    "    Compute nth-order structure functions using vectorized NumPy operations.\n",
    "    Supports both 1D (single time series or profile) and 2D (multiple series/profiles) input.\n",
    "    \n",
    "    Parameters:\n",
    "        field (1D or 2D array): Input data, where structure functions are computed along last axis.\n",
    "        max_lag (int): Maximum lag (spatial or temporal separation).\n",
    "        orders (list of int): List of orders (moments) to compute.\n",
    "        \n",
    "    Returns:\n",
    "        dict: Keys are order integers, values are arrays of shape (max_lag,) or (n_series, max_lag)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Convert input to NumPy array\n",
    "    field = np.asarray(field)\n",
    "    \n",
    "    # If field is 1D, convert to 2D with one row for uniform processing\n",
    "    if field.ndim == 1:\n",
    "        field = field[np.newaxis, :]\n",
    "\n",
    "    # Get number of series and number of points per series\n",
    "    n_series, n_points = field.shape\n",
    "\n",
    "    # Initialize dictionary to hold structure functions\n",
    "    S = {}\n",
    "\n",
    "    # Initialize 3D array of shape (max_lag, n_series, effective_length) for differences\n",
    "    diffs = np.array([\n",
    "        field[:, lag:] - field[:, :-lag]\n",
    "        for lag in range(1, max_lag + 1)\n",
    "    ])  # shape: (max_lag, n_series, n_points - lag)\n",
    "\n",
    "    # Loop over each order\n",
    "    for order in orders:\n",
    "        # Compute mean over the last axis (lags), preserve shape (max_lag, n_series)\n",
    "        S_order = np.mean(np.abs(diffs) ** order, axis=2)\n",
    "\n",
    "        # Transpose to shape (n_series, max_lag) for clarity\n",
    "        S[order] = S_order.T if n_series > 1 else S_order.flatten()\n",
    "\n",
    "    # Return dictionary of structure functions\n",
    "    return S\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airsea_coupling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
