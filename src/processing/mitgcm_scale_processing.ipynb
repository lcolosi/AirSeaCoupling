{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing MITGCM data for Scale Analysis \n",
    "\n",
    "**Purpose**: Code for producing data for scale analysis. I will be producing three types of data output for temperature and salinity: \n",
    "\n",
    "1. Point measurements on, off, and in the transition region of the shelf. \n",
    "2. Zonal and meridional spatial transects on, off, and in the transition region of the shelf. \n",
    "\n",
    "For statistical stability, I should average the autocorrelation functions over multiple points or multiple spatial transects. \n",
    "\n",
    "**Luke Colosi | lcolosi@ucsd.edu**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Force matplotlib plots to display directly within the output cell of the notebook: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import xarray as xr\n",
    "from xmitgcm import open_mdsdataset\n",
    "\n",
    "#--- Other Functions ---# \n",
    "sys.path.append(\"/home/lcolosi/AirSeaCoupling/tools/\")\n",
    "import cartopy_figs as cart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set data analysis parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters \n",
    "delta_t = 150  # Time steps of the raw model run (by raw, I mean the time increments that the model is ran at, not time increments that the diagnostics are output at). Units: seconds\n",
    "\n",
    "# Set time and space parameters  \n",
    "lat_bnds  = [32.5, 35]                                           # Specifies the latitude bounds for the region to analyze\n",
    "lon_bnds  = [236.0, 241.0]                                       # Specifies the longitude bounds for the region to analyze\n",
    "encoding  = {'time': {'units': 'seconds since 2015-12-01 2:00'}} # Specifies the start time of the model run\n",
    "\n",
    "# Set path to project directory\n",
    "PATH_GRID   = '/data/SO2/SWOT/GRID/BIN/'                    # Space and time grid of the model \n",
    "PATH_OUTPUT = '/data/SO2/SWOT/MARA/RUN4_LY/DIAGS_HRLY/'     # Diagnostics of the model\n",
    "PATH_nc     = '/data/SO3/lcolosi/mitgcm/SWOT_MARA_RUN4_LY/'  # Directory to save netCDFs \n",
    "PATH_figs   = '/home/lcolosi/AirSeaCoupling/figs_server/mitgcm/preliminary/'\n",
    "file_dim    = '3D'                                          # Set the dimension of the data (to include the depth or not)\n",
    "\n",
    "# Set plotting parameters \n",
    "fontsize = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the grid and diagnostics data into a python structure. The diagnostics that we will be looking at include: \n",
    "\n",
    "1. **Potential Temperature** $\\theta$: $^\\circ C$\n",
    "2. **Salinity** $S$: $g/kg$\n",
    "3. **Stratification** $\\frac{d\\sigma}{dz}$: $kg/m^4$\n",
    "4. **Zonal, meridional, and vertical velocity components**  $\\textbf{u} = (u,v,w)$: $m/s$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset \n",
    "ds = open_mdsdataset(\n",
    "    PATH_OUTPUT,                    # File path where the model output data is stored (.data and .meta files)\n",
    "    PATH_GRID,                      # File path to the grid data of the model \n",
    "    iters='all',                    # Specifies which iteration of the data to load\n",
    "    delta_t=delta_t, \n",
    "    ignore_unknown_vars=False,      # Specifies whether to ignore any unknown variables that may appear in the dataset\n",
    "    prefix=['diags_' + file_dim],   # List of prefixes to filter the variables in the dataset\n",
    "    ref_date=\"2015-01-01 02:00:00\", # Specifies the starting point of the simulation time (which may include the spin up time before diagnostics are output)\n",
    "    geometry='sphericalpolar',      # Specifies the  grid's geometry is spherical-polar. \n",
    "    chunks={'i': 48, 'i_g': 48, 'j': 56,  'j_g': 56, 'k': 10, 'k_u':10, 'k_l':10, 'k_p1':10} # Chunck data upload into 48x56 pieces (spatial grid is divided into 12 chunks along the x and y axes) chunks={'i': 48, 'i_g': 48, 'j': 56,  'j_g': 56, 'k': 10, 'k_u':10, 'k_l':10, 'k_p1':10, 'time':1} chunks={'XC':48, 'YC':56, 'XG':48, 'YG':56, 'Z':10, 'Zp1':10, 'Zu':10, 'Zl':10, 'time':1}\n",
    ")\n",
    "\n",
    "# Convert all variables and coordinates in the dataset to little-endian (the format how multi-byte values are stored into memory)\n",
    "\n",
    "#--- Variables ---#\n",
    "for var in ds.data_vars:\n",
    "    if ds[var].dtype.byteorder == '>' or (ds[var].dtype.byteorder == '=' and sys.byteorder == \"big\"):  # Check if big-endian\n",
    "        ds[var] = ds[var].astype(ds[var].dtype.newbyteorder('<'))\n",
    "\n",
    "#--- Coordinates ---# \n",
    "for coord in ds.coords:\n",
    "    if ds[coord].dtype.byteorder == '>'or (ds[coord].dtype.byteorder == '=' and sys.byteorder == \"big\"):  # Check if big-endian\n",
    "        ds[coord] = ds[coord].astype(ds[coord].dtype.newbyteorder('<'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slice the array based on latitude and longitude bounds and select the depth levels to analyze. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_subset = ds.sel(\n",
    "    XG=slice(*lon_bnds),\n",
    "    YG=slice(*lat_bnds),\n",
    "    XC=slice(*lon_bnds),\n",
    "    YC=slice(*lat_bnds),\n",
    ").isel(Z=0, Zl=0, Zp1=0, Zu=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slice dataset for autocorrelation analysis in the following manner: \n",
    "\n",
    "1. Select a longitude and latitude point and get the time series at the point. For statistical convergence, grab time series of nearby longitude and latitude points (3x3 grid) \n",
    "\n",
    "2. Select spatial transects at one time step along a line of constant longitude and a line of constant latitude. For statistcal convergence, select the same transects for all time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the slicing parameters (looking just on the offshelf for the present)\n",
    "longitude_point = -122 % 360\n",
    "latitude_point = 34\n",
    "longitude = -122 % 360\n",
    "latitude_b = [33, 34.25]\n",
    "latitude = 34\n",
    "longitude_b = [-123 % 360 , -121.75 % 360]\n",
    "\n",
    "# Get the time series at a specific point and its adjacent points\n",
    "ds_ts = ds_subset.sel(XG=slice(longitude_point-1, longitude_point+2), \n",
    "                      YG=slice(latitude_point-1, latitude_point+2),\n",
    "                      XC=slice(longitude_point-1, longitude_point+2), \n",
    "                      YC=slice(latitude_point-1, latitude_point+2))\n",
    "\n",
    "# Grab a transect at a constant longitude for all time steps\n",
    "ds_nearest_lon = ds_subset.sel(XC=longitude, method='nearest')\n",
    "ds_trans_zonal = ds_nearest_lon.sel(YC=slice(*latitude_b))\n",
    "\n",
    "# Grab a transect at a constant latitude for all time steps\n",
    "ds_nearest_lat = ds_subset.sel(YC=latitude, method='nearest')\n",
    "ds_trans_meridional = ds_subset.sel(XC=slice(*longitude_b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lcolosi/miniconda3/envs/ccs_scale/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:585: UserWarning: endian-ness of dtype and endian kwarg do not match, using endian kwarg\n",
      "  nc4_var = self.ds.createVariable(**default_args)\n",
      "/home/lcolosi/miniconda3/envs/ccs_scale/lib/python3.12/site-packages/xarray/backends/netCDF4_.py:585: UserWarning: endian-ness of dtype and endian kwarg do not match, using endian kwarg\n",
      "  nc4_var = self.ds.createVariable(**default_args)\n"
     ]
    }
   ],
   "source": [
    "#--- Point Measurement Variables ---# \n",
    "# ds_ts['THETA'].to_netcdf(PATH_nc + 'THETA_CCS4_hrly_ts_ext.nc', encoding=encoding)\n",
    "ds_ts['SALT'].to_netcdf(PATH_nc + 'SALT_CCS4_hrly_ts_ext.nc', encoding=encoding)\n",
    "\n",
    "ds_ts['UVEL'].to_netcdf(PATH_nc + 'UVEL_CCS4_hrly_ts_ext.nc', encoding=encoding)\n",
    "#ds_ts['VVEL'].to_netcdf(PATH_nc + 'VVEL_CCS4_hrly_ts_ext.nc', encoding=encoding)\n",
    "ds_ts['WVEL'].to_netcdf(PATH_nc + 'WVEL_CCS4_hrly_ts_ext.nc', encoding=encoding)\n",
    "\n",
    "# ds_ts['DRHODR'].to_netcdf(PATH_nc + 'Strat_CCS4_hrly_ts.nc', encoding=encoding)\n",
    "\n",
    "# #--- Zonal Variables ---# \n",
    "# ds_trans_zonal['THETA'].to_netcdf(PATH_nc + 'THETA_CCS4_hrly_zonal.nc', encoding=encoding)\n",
    "# ds_trans_zonal['SALT'].to_netcdf(PATH_nc + 'SALT_CCS4_hrly_zonal.nc', encoding=encoding)\n",
    "\n",
    "# ds_trans_zonal['UVEL'].to_netcdf(PATH_nc + 'UVEL_CCS4_hrly_zonal.nc', encoding=encoding)\n",
    "# ds_trans_zonal['VVEL'].to_netcdf(PATH_nc + 'VVEL_CCS4_hrly_zonal.nc', encoding=encoding)\n",
    "# ds_trans_zonal['WVEL'].to_netcdf(PATH_nc + 'WVEL_CCS4_hrly_zonal.nc', encoding=encoding)\n",
    "\n",
    "# ds_trans_zonal['DRHODR'].to_netcdf(PATH_nc + 'Strat_CCS4_hrly_zonal.nc', encoding=encoding)\n",
    "\n",
    "# #--- Meridional Variables ---# \n",
    "# ds_trans_meridional['THETA'].to_netcdf(PATH_nc + 'THETA_CCS4_hrly_meridional.nc', encoding=encoding)\n",
    "# ds_trans_meridional['SALT'].to_netcdf(PATH_nc + 'SALT_CCS4_hrly_meridional.nc', encoding=encoding)\n",
    "\n",
    "# ds_trans_meridional['UVEL'].to_netcdf(PATH_nc + 'UVEL_CCS4_hrly_meridional.nc', encoding=encoding)\n",
    "# ds_trans_meridional['VVEL'].to_netcdf(PATH_nc + 'VVEL_CCS4_hrly_meridional.nc', encoding=encoding)\n",
    "# ds_trans_meridional['WVEL'].to_netcdf(PATH_nc + 'WVEL_CCS4_hrly_meridional.nc', encoding=encoding)\n",
    "\n",
    "# ds_trans_meridional['DRHODR'].to_netcdf(PATH_nc + 'Strat_CCS4_hrly_meridional.nc', encoding=encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "airsea_coupling",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
