{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bce4a1c",
   "metadata": {},
   "source": [
    "# Processing MITGCM data for Scale Analysis \n",
    "\n",
    "**Purpose**: Code for producing data for temporal decorrelation scale analysis. Here, we will generate time series of temperature, salinity, and velocity profiles at the locations of the CCE1, CCE2, and CCE3 moorings. \n",
    "\n",
    "**Luke Colosi | lcolosi@ucsd.edu**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecc58f8",
   "metadata": {},
   "source": [
    "Force matplotlib plots to display directly within the output cell of the notebook: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d4bd645",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff45b56d",
   "metadata": {},
   "source": [
    "Import python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c9a8cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from xmitgcm import open_mdsdataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c800b7f",
   "metadata": {},
   "source": [
    "Set data analysis parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "99b32142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters \n",
    "delta_t = 150  # Time steps of the raw model run (by raw, I mean the time increments that the model is ran at, not time increments that the diagnostics are output at). Units: seconds\n",
    "\n",
    "# Set time and space parameters  \n",
    "lat_cce  = [33.457, 34.3075, 34.44825228022894]                  # Specifies the latitude bounds for the region to analyze\n",
    "lon_cce  = [-122.52233, -120.8042, -120.53825701527784]          # Specifies the longitude bounds for the region to analyze\n",
    "encoding  = {'time': {'units': 'seconds since 2015-12-01 2:00'}} # Specifies the start time of the model run\n",
    "\n",
    "# Set path to project directory\n",
    "PATH_GRID   = '/data/SO2/SWOT/GRID/BIN/'                    # Space and time grid of the model \n",
    "PATH_OUTPUT = '/data/SO2/SWOT/MARA/RUN4_LY/DIAGS_HRLY/'     # Diagnostics of the model\n",
    "PATH_nc     = '/data/SO3/lcolosi/mitgcm/SWOT_MARA_RUN4_LY/' # Directory to save netCDFs \n",
    "file_dim    = '3D'                                          # Set the dimension of the data (to include the depth or not where 3D is T,S,drhodr,vel and 2D is etan)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc570234",
   "metadata": {},
   "source": [
    "Load the grid and diagnostics data into a python structure. The diagnostics that we will be looking at include: \n",
    "\n",
    "1. **Potential Temperature** $\\theta$: $^\\circ C$\n",
    "2. **Salinity** $S$: $g/kg$\n",
    "3. **Stratification** $\\frac{d\\sigma}{dz}$: $kg/m^4$\n",
    "4. **Zonal, meridional, and vertical velocity components**  $\\textbf{u} = (u,v,w)$: $m/s$  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72e7036b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset \n",
    "ds = open_mdsdataset(\n",
    "    PATH_OUTPUT,                    # File path where the model output data is stored (.data and .meta files)\n",
    "    PATH_GRID,                      # File path to the grid data of the model \n",
    "    iters='all',                    # Specifies which iteration of the data to load\n",
    "    delta_t=delta_t, \n",
    "    ignore_unknown_vars=False,      # Specifies whether to ignore any unknown variables that may appear in the dataset\n",
    "    prefix=['diags_' + file_dim],   # List of prefixes to filter the variables in the dataset\n",
    "    ref_date=\"2015-01-01 02:00:00\", # Specifies the starting point of the simulation time (which may include the spin up time before diagnostics are output)\n",
    "    geometry='sphericalpolar'       # Specifies the  grid's geometry is spherical-polar. \n",
    ")\n",
    "\n",
    "# Convert all variables and coordinates in the dataset to little-endian (the format how multi-byte values are stored into memory)\n",
    "\n",
    "#--- Variables ---#\n",
    "for var in ds.data_vars:\n",
    "    if ds[var].dtype.byteorder == '>' or (ds[var].dtype.byteorder == '=' and sys.byteorder == \"big\"):  # Check if big-endian\n",
    "        ds[var] = ds[var].astype(ds[var].dtype.newbyteorder('<'))\n",
    "\n",
    "#--- Coordinates ---# \n",
    "for coord in ds.coords:\n",
    "    if ds[coord].dtype.byteorder == '>'or (ds[coord].dtype.byteorder == '=' and sys.byteorder == \"big\"):  # Check if big-endian\n",
    "        ds[coord] = ds[coord].astype(ds[coord].dtype.newbyteorder('<'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da97116",
   "metadata": {},
   "source": [
    "Slice array based on longitude and latitude of CCE moorings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baa44c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 2D coordinate fields\n",
    "lat_YC = ds['YC']\n",
    "lon_XC = ds['XC']\n",
    "lat_YG = ds['YG']\n",
    "lon_XG = ds['XG']\n",
    "\n",
    "# Set dictionary to hold extracted profiles per variable\n",
    "all_profiles = {var: [] for var in ds.data_vars}\n",
    "\n",
    "# Loop through CCE sites\n",
    "for i, (lat_target, lon_target) in enumerate(zip(lat_cce, lon_cce)):\n",
    "    \n",
    "    # --- For center-point variables (XC/YC) ---\n",
    "    dist_sq_center = (lat_YC - lat_target)**2 + (lon_XC - lon_target)**2\n",
    "    j_YC, i_XC = np.unravel_index(np.argmin(dist_sq_center.values), dist_sq_center.shape)\n",
    "    \n",
    "    # --- For U-point variables (XG/YC) ---\n",
    "    dist_sq_u = (lat_YC - lat_target)**2 + (lon_XG - lon_target)**2\n",
    "    j_YC_u, i_XG = np.unravel_index(np.argmin(dist_sq_u.values), dist_sq_u.shape)\n",
    "    \n",
    "    # --- For V-point variables (XC/YG) ---\n",
    "    dist_sq_v = (lat_YG - lat_target)**2 + (lon_XC - lon_target)**2\n",
    "    j_YG, i_XC_v = np.unravel_index(np.argmin(dist_sq_v.values), dist_sq_v.shape)\n",
    "    \n",
    "    # --- Loop through all variables and select appropriate index ---\n",
    "    for var in ds.data_vars:\n",
    "        da = ds[var]\n",
    "\n",
    "        if {'YC', 'XC'}.issubset(da.dims):\n",
    "            sel = da.isel(YC=j_YC, XC=i_XC)\n",
    "        elif {'YC', 'XG'}.issubset(da.dims):\n",
    "            sel = da.isel(YC=j_YC_u, XG=i_XG)\n",
    "        elif {'YG', 'XC'}.issubset(da.dims):\n",
    "            sel = da.isel(YG=j_YG, XC=i_XC_v)\n",
    "        else:\n",
    "            # For unexpected variable shapes\n",
    "            print(f\"Skipping {var}: unknown coordinate configuration.\")\n",
    "            continue\n",
    "        \n",
    "        # Add site dimension\n",
    "        sel = sel.expand_dims(site=[f\"CCE{i+1}\"])\n",
    "        all_profiles[var].append(sel)\n",
    "\n",
    "# --- Combine all variables into datasets per variable ---\n",
    "profiles_ds = xr.Dataset({var: xr.concat(all_profiles[var], dim='site') for var in all_profiles})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b7e07b",
   "metadata": {},
   "source": [
    "Save data into a netcdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2676f561",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving THETA...\n",
      "Saving SALT...\n"
     ]
    }
   ],
   "source": [
    "# Loop through each variable in the profiles dataset\n",
    "for var in profiles_ds.data_vars:\n",
    "    \n",
    "    # Select the data array corresponding to the current variable\n",
    "    da = profiles_ds[var]\n",
    "    \n",
    "    # Rechunk the data for faster writing:\n",
    "    # Here we chunk along the 'time' dimension (adjust the chunk size as needed)\n",
    "    da = da.chunk({'time': 1000})\n",
    "    \n",
    "    # Print status to monitor progress\n",
    "    print(f\"Saving {var}...\")\n",
    "\n",
    "    # Load the data into memory before saving (forces Dask to compute first),\n",
    "    # which can speed up the actual file writing and reduce errors.\n",
    "    da = da.load()\n",
    "    \n",
    "    # Save the data array to a NetCDF file\n",
    "    da.to_netcdf(\n",
    "        f\"{PATH_nc}{var}_CCS4_hrly_ts_profile.nc\",  # Output file name\n",
    "        engine='scipy',                             # Use the faster 'scipy' NetCDF writer\n",
    "        format='NETCDF3_64BIT',                     # Use a simpler NetCDF format (faster & widely compatible)\n",
    "        encoding=encoding\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ccs_scale",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
